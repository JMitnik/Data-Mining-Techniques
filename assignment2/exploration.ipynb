{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Setting up\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import Lasso, RidgeClassifier, LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, RFE\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config parameters:\n",
    "\n",
    "nrows: default dataframelength: 4958347\n",
    "\n",
    "classifiers: (SVC, linearSVC, RidgeClassifier)\n",
    "    - SVC params: C (0-1), kernel (rbf, linear, poly), max_iter (-1, int), random_state (int)\n",
    "    - linearSVC params: C (0-1), penalty (l1, l2), max_iter (1000, int), random_state (int) \n",
    "    - RidgeClassifier params: max_iter (1000, int), random_state (int) \n",
    "    \n",
    "feature_selection: (SelectFromModel, SelectKBest, RFE)\n",
    "    - SelectFromModel params: threshold (0-int), max_features (0-int)\n",
    "    - SelectKBest params: threshold (0-int), k (0-int)\n",
    "    - RFE params: n_features_to_select (0-int), step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import config\n",
    "importlib.reload(config)\n",
    "from config import Config\n",
    "\n",
    "# Config Settings\n",
    "config = Config(\n",
    "    nrows=1000,\n",
    "    pre_feature_selection=False,\n",
    "    train_data_subset=0.8,\n",
    "    classifier=SVC,\n",
    "    classifier_dict={'C' : 1, 'kernel' : 'rbf', 'random_state' : 2},\n",
    "    feature_selection=SelectFromModel,\n",
    "    feature_selection_dict={'threshold' : 1},\n",
    "    dimensionality_reduc=True,\n",
    "    dimension_features=25,\n",
    "    feature_engineering=True,  \n",
    "    naive_imputing=True #todo faster method for averaging nan values if naive=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>date_time</th>\n",
       "      <th>site_id</th>\n",
       "      <th>visitor_location_country_id</th>\n",
       "      <th>visitor_hist_starrating</th>\n",
       "      <th>visitor_hist_adr_usd</th>\n",
       "      <th>prop_country_id</th>\n",
       "      <th>prop_id</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>...</th>\n",
       "      <th>comp6_rate_percent_diff</th>\n",
       "      <th>comp7_rate</th>\n",
       "      <th>comp7_inv</th>\n",
       "      <th>comp7_rate_percent_diff</th>\n",
       "      <th>comp8_rate</th>\n",
       "      <th>comp8_inv</th>\n",
       "      <th>comp8_rate_percent_diff</th>\n",
       "      <th>click_bool</th>\n",
       "      <th>gross_bookings_usd</th>\n",
       "      <th>booking_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>10404</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>21315</td>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>27348</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>29604</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   srch_id            date_time  site_id  visitor_location_country_id  \\\n",
       "0        1  2013-04-04 08:32:15       12                          187   \n",
       "1        1  2013-04-04 08:32:15       12                          187   \n",
       "2        1  2013-04-04 08:32:15       12                          187   \n",
       "3        1  2013-04-04 08:32:15       12                          187   \n",
       "4        1  2013-04-04 08:32:15       12                          187   \n",
       "\n",
       "   visitor_hist_starrating  visitor_hist_adr_usd  prop_country_id  prop_id  \\\n",
       "0                      NaN                   NaN              219      893   \n",
       "1                      NaN                   NaN              219    10404   \n",
       "2                      NaN                   NaN              219    21315   \n",
       "3                      NaN                   NaN              219    27348   \n",
       "4                      NaN                   NaN              219    29604   \n",
       "\n",
       "   prop_starrating  prop_review_score  ...  comp6_rate_percent_diff  \\\n",
       "0                3                3.5  ...                      NaN   \n",
       "1                4                4.0  ...                      NaN   \n",
       "2                3                4.5  ...                      NaN   \n",
       "3                2                4.0  ...                      NaN   \n",
       "4                4                3.5  ...                      NaN   \n",
       "\n",
       "   comp7_rate  comp7_inv  comp7_rate_percent_diff  comp8_rate  comp8_inv  \\\n",
       "0         NaN        NaN                      NaN         0.0        0.0   \n",
       "1         NaN        NaN                      NaN         0.0        0.0   \n",
       "2         NaN        NaN                      NaN         0.0        0.0   \n",
       "3         NaN        NaN                      NaN        -1.0        0.0   \n",
       "4         NaN        NaN                      NaN         0.0        0.0   \n",
       "\n",
       "   comp8_rate_percent_diff  click_bool  gross_bookings_usd  booking_bool  \n",
       "0                      NaN           0                 NaN             0  \n",
       "1                      NaN           0                 NaN             0  \n",
       "2                      NaN           0                 NaN             0  \n",
       "3                      5.0           0                 NaN             0  \n",
       "4                      NaN           0                 NaN             0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if config.nrows is not None:\n",
    "    train_data = pd.read_csv('data/training_set_VU_DM.csv', nrows=config.nrows)\n",
    "else:\n",
    "    train_data = pd.read_csv('data/training_set_VU_DM.csv')\n",
    "original_columns = train_data.columns\n",
    "train_data.head(5) # Show top 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Column exploration\n",
    "---\n",
    "## Main columns\n",
    "- `search_id` seems to represent each individual 'user'.\n",
    "- `booking_bool` is essentially the answer.\n",
    "\n",
    "## Categorical features\n",
    "The following features are categorical (to be onehot-encoded):\n",
    "\n",
    "User-specific\n",
    "- `site_id`: category of website Expedia used\n",
    "- `visitor_location_country_id`: categories of which country user is from\n",
    "- `srch_destination_id`: where did the user search from\n",
    "- `srch_saturday_night_bool`: boolean if stay includes staturday\n",
    "\n",
    "Hotel-specific:\n",
    "- `prop_id`: categories of associated hotels\n",
    "- `prop_brand_bool`: boolean if hotel is part of chain or not\n",
    "- `promotion_flag`: displaying promotion or not\n",
    "\n",
    "Expedia-specific vs competitors 1_8:\n",
    "- `comp{i}_rate`: if expedia has a lower price, do +1, 0 if same, -1 price is higher, null if no competitive data\n",
    "- `comp{i}_inv`: if competitor has no availability, +1, 0 if both have availability, null if no competitive data\n",
    "\n",
    "## Numerical features\n",
    "\n",
    "User-specific\n",
    "- `visitor_hist_starrating`: average of previous stars of associated user\n",
    "- `visitor_hist_adr_usd`: average price per night of hotels of associated user\n",
    "- `srch_length_of_stay`: number of nights stays **searched** \n",
    "- `srch_booking_window`: number of days ahead the start of booking window **searched**\n",
    "- `srch_adults_count`: number of adults **searched**\n",
    "- `srch_children_count`: number of children **searched**\n",
    "- `srch_room_count`: number of rooms **searched**\n",
    "- `random_bool`: if sort was random at time of search\n",
    "- `gross_booking_usd`: ❗Training-only❗ payment includign taxes, etc for hotel\n",
    "\n",
    "Hotel-specific\n",
    "- `prop_starrating`: star rating of hotel (1-5)\n",
    "- `prop_review_score`: average review score of hotel (1-5)\n",
    "- `prop_location_score_1`: score1 of hotel's location desirability\n",
    "- `prop_location_score_2`: score2 of hotel's location desirability\n",
    "- `prop_log_historical_price`: logarithm of average price of hotel lately (0 == not sold)\n",
    "- `price_usd`: displayed price of hotel.\n",
    "    - ❗ Important: Different countries have different conventions.\n",
    "    - Value can change per night\n",
    "- `srch_query_affinity_score`: log probability a hotel is clicked in internet searches\n",
    "\n",
    "User-hotel coupled:\n",
    "- `orig_destination_distance`: distance between hotel and customer at search-time (null means no distance calculated)\n",
    "\n",
    "Expedia-specific vs competitors 1_8:\n",
    "- `comp{i}_rate_percent_diff`: absolute difference between expedia and competitor's price, with null being no competitive data\n",
    "\n",
    "\n",
    "## Unknown type\n",
    "- `date_time`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all columns except competitor columns in either categorical list or numerical list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['srch_id', 'date_time', 'site_id', 'visitor_location_country_id', 'prop_country_id',\n",
    "                    'prop_id', 'prop_brand_bool', 'promotion_flag', 'position',\n",
    "                    'srch_destination_id', 'srch_saturday_night_bool', 'random_bool',\n",
    "                    'click_bool', 'booking_bool'                  \n",
    "                   ]\n",
    "numerical_cols = ['visitor_hist_starrating', 'visitor_hist_adr_usd', \n",
    "                  'prop_starrating', 'prop_review_score', 'prop_location_score1', 'prop_location_score2',\n",
    "                  'prop_log_historical_price', 'price_usd', \n",
    "                  'srch_length_of_stay', 'srch_booking_window', \n",
    "                  'srch_adults_count', 'srch_children_count',\n",
    "                  'srch_room_count', 'srch_query_affinity_score', \n",
    "                  'orig_destination_distance', 'gross_bookings_usd'\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When engineering we want to add them to either the numerical or categorical columnlist and append them to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.feature_engineering:\n",
    "\n",
    "    time = pd.to_datetime(train_data['date_time'])\n",
    "    train_data['month']=time.dt.month\n",
    "    train_data['year']=time.dt.year\n",
    "    train_data['same_country_visitor_prop']=np.where(train_data['visitor_location_country_id'] == train_data['prop_country_id'],1,0)\n",
    "    train_data['viable_comp']= np.where(\n",
    "                      (train_data['comp1_rate']== -1)& (train_data['comp1_inv']== 0) |\n",
    "                      (train_data['comp2_rate']== -1)& (train_data['comp2_inv']== 0) |\n",
    "                      (train_data['comp3_rate']== -1)& (train_data['comp3_inv']== 0) |\n",
    "                      (train_data['comp4_rate']== -1)& (train_data['comp4_inv']== 0) |\n",
    "                      (train_data['comp5_rate']== -1)& (train_data['comp5_inv']== 0) |\n",
    "                      (train_data['comp6_rate']== -1)& (train_data['comp6_inv']== 0) |\n",
    "                      (train_data['comp7_rate']== -1)& (train_data['comp7_inv']== 0) |\n",
    "                      (train_data['comp8_rate']== -1)& (train_data['comp8_inv']== 0) \n",
    "                      ,1,0)\n",
    "\n",
    "    mcol=train_data.loc[:,['prop_location_score1', 'prop_location_score2']]\n",
    "    train_data['prop_mean_score'] = mcol.mean(axis=1)\n",
    "    categorical_engi = ['same_country_visitor_prop', 'viable_comp']\n",
    "    numerical_engi = ['prop_mean_score', 'month', 'year']\n",
    "    for col in categorical_engi:\n",
    "        categorical_cols.append(col)\n",
    "    for col in numerical_engi:\n",
    "        numerical_cols.append(col)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srch_id int64\n",
      "date_time object\n",
      "site_id int64\n",
      "visitor_location_country_id int64\n",
      "visitor_hist_starrating float64\n",
      "visitor_hist_adr_usd float64\n",
      "prop_country_id int64\n",
      "prop_id int64\n",
      "prop_starrating int64\n",
      "prop_review_score float64\n",
      "prop_brand_bool int64\n",
      "prop_location_score1 float64\n",
      "prop_location_score2 float64\n",
      "prop_log_historical_price float64\n",
      "position int64\n",
      "price_usd float64\n",
      "promotion_flag int64\n",
      "srch_destination_id int64\n",
      "srch_length_of_stay int64\n",
      "srch_booking_window int64\n",
      "srch_adults_count int64\n",
      "srch_children_count int64\n",
      "srch_room_count int64\n",
      "srch_saturday_night_bool int64\n",
      "srch_query_affinity_score float64\n",
      "orig_destination_distance float64\n",
      "random_bool int64\n",
      "comp1_rate float64\n",
      "comp1_inv float64\n",
      "comp1_rate_percent_diff float64\n",
      "comp2_rate float64\n",
      "comp2_inv float64\n",
      "comp2_rate_percent_diff float64\n",
      "comp3_rate float64\n",
      "comp3_inv float64\n",
      "comp3_rate_percent_diff float64\n",
      "comp4_rate float64\n",
      "comp4_inv float64\n",
      "comp4_rate_percent_diff float64\n",
      "comp5_rate float64\n",
      "comp5_inv float64\n",
      "comp5_rate_percent_diff float64\n",
      "comp6_rate float64\n",
      "comp6_inv float64\n",
      "comp6_rate_percent_diff float64\n",
      "comp7_rate float64\n",
      "comp7_inv float64\n",
      "comp7_rate_percent_diff float64\n",
      "comp8_rate float64\n",
      "comp8_inv float64\n",
      "comp8_rate_percent_diff float64\n",
      "click_bool int64\n",
      "gross_bookings_usd float64\n",
      "booking_bool int64\n",
      "month int64\n",
      "year int64\n",
      "same_country_visitor_prop int32\n",
      "viable_comp int32\n",
      "prop_mean_score float64\n"
     ]
    }
   ],
   "source": [
    "for column in train_data.columns:\n",
    "    print (column, train_data[column].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we engineer new features, we might want to remove their old columns from the dataframe and columnlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.feature_engineering:\n",
    "    train_data = train_data.drop(columns=['date_time', 'visitor_location_country_id', 'prop_country_id', \n",
    "                             'prop_location_score1', 'prop_location_score2'])\n",
    "    for i in range(8):\n",
    "        train_data = train_data.drop(columns=['comp' + str(i+1) + '_rate'])\n",
    "        train_data = train_data.drop(columns=['comp' + str(i+1) + '_inv'])\n",
    "        train_data = train_data.drop(columns=['comp' + str(i+1) + '_rate_percent_diff'])\n",
    "    categorical_to_remove = ['visitor_location_country_id', 'date_time', 'prop_country_id']\n",
    "    numerical_to_remove = ['prop_location_score1', 'prop_location_score2']\n",
    "    \n",
    "    for col in categorical_to_remove:\n",
    "        categorical_cols.remove(col)\n",
    "    for col in numerical_to_remove:\n",
    "        numerical_cols.remove(col)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleanup: Imputing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['visitor_hist_starrating', 'visitor_hist_adr_usd', 'prop_review_score',\n",
       "       'srch_query_affinity_score', 'orig_destination_distance',\n",
       "       'gross_bookings_usd'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will have to cleanup our data next up. Let's first impute the missing columns. \n",
    "# To do this we search for the columns with nans\n",
    "na_cols = train_data.isna().any()\n",
    "nan_cols = train_data.columns[na_cols]\n",
    "nan_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from `comp{i}_rate` and `comp2_inv`, all of these columns are numerical features. We could, initially,\n",
    "simply replace all these values with -1 for the moment.\n",
    "\n",
    "❗ Important: Note, this is actually incorrect, but might work for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple numerical impute: select numerical data, fill it with -1\n",
    "if config.naive_imputing:    \n",
    "    imputed_numerical_data = train_data[nan_cols].filter(regex='[^comp\\d_(rate|inv)$]')\n",
    "    imputed_numerical_data = imputed_numerical_data.fillna(-1)\n",
    "    train_data.update(imputed_numerical_data)\n",
    "\n",
    "    # Manual cleanup to ensure no problem with space\n",
    "    del imputed_numerical_data\n",
    "    train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Simple naive categorical impute\n",
    "if config.naive_imputing:    \n",
    "    na_cols = train_data.columns[train_data.isna().any()]\n",
    "    imputed_categorical_data = train_data[na_cols].fillna(-2)\n",
    "    train_data.update(imputed_categorical_data)\n",
    "\n",
    "    # Cleanup\n",
    "    del imputed_categorical_data\n",
    "    train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second, less naive approach is to average numerical values grouped by either their hotel (prop_id) or the user (srch_id).\n",
    "On top of that we would want to remove columns with over 50% null Values (refence for this?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "srch_id                      0.0\n",
       "site_id                      0.0\n",
       "visitor_hist_starrating      0.0\n",
       "visitor_hist_adr_usd         0.0\n",
       "prop_id                      0.0\n",
       "prop_starrating              0.0\n",
       "prop_review_score            0.0\n",
       "prop_brand_bool              0.0\n",
       "prop_log_historical_price    0.0\n",
       "position                     0.0\n",
       "price_usd                    0.0\n",
       "promotion_flag               0.0\n",
       "srch_destination_id          0.0\n",
       "srch_length_of_stay          0.0\n",
       "srch_booking_window          0.0\n",
       "srch_adults_count            0.0\n",
       "srch_children_count          0.0\n",
       "srch_room_count              0.0\n",
       "srch_saturday_night_bool     0.0\n",
       "srch_query_affinity_score    0.0\n",
       "orig_destination_distance    0.0\n",
       "random_bool                  0.0\n",
       "click_bool                   0.0\n",
       "gross_bookings_usd           0.0\n",
       "booking_bool                 0.0\n",
       "month                        0.0\n",
       "year                         0.0\n",
       "same_country_visitor_prop    0.0\n",
       "viable_comp                  0.0\n",
       "prop_mean_score              0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove columns with over 50% nans\n",
    "if config.naive_imputing == False:\n",
    "\n",
    "    for column in train_data.columns:\n",
    "        if train_data[column].isnull().sum()/len(train_data) > 0.5:\n",
    "            train_data = train_data.drop(columns=column, axis=1)\n",
    "\n",
    "train_data.isnull().sum()/len(train_data)\n",
    "    #remove data with > 0.50 nans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slow method of averaging mean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.naive_imputing == False:\n",
    "\n",
    "    #fill in nans with mean values:\n",
    "    na_cols = train_data.isna().any()\n",
    "    nan_cols = train_data.columns[na_cols]\n",
    "    for column in nan_cols:\n",
    "        print (column)\n",
    "        if column in ['visitor_hist_starrating', 'visitor_hist_adr_usd',  \n",
    "                         'srch_length_of_stay', 'srch_booking_window', \n",
    "                         'srch_adults_count', 'srch_children_count',\n",
    "                         'srch_room_count'                      \n",
    "                        ]:\n",
    "            train_data[column] = train_data.groupby('srch_id').transform(lambda x: x.fillna(x.mean()))\n",
    "        elif column in ['prop_starrating', 'prop_review_score', \n",
    "                           'prop_location_score1', 'prop_location_score2', \n",
    "                           'prop_log_historical_price', 'price_usd',\n",
    "                           'search_', 'orig_destination_distance',  \n",
    "                           'srch_query_affinity_score'\n",
    "                          ]:\n",
    "            train_data[column] = train_data.groupby('prop_id').transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "    train_data.isnull().sum()/len(train_data)\n",
    "    \n",
    "if config.naive_imputing == True:\n",
    "        # Here we definehow we would like to encode\n",
    "\n",
    "    # For One-Hot Encoding\n",
    "    # Onehot encode the categorical variables\n",
    "    oh_impute = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=-2)\n",
    "    oh_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    oh_pipeline = Pipeline([\n",
    "        ('impute', oh_impute),\n",
    "        ('encode', oh_encoder)\n",
    "    ])\n",
    "\n",
    "    # Encode the numerical values\n",
    "    num_impute = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=-1)\n",
    "    num_scale_encoder = StandardScaler()\n",
    "    num_pipeline = Pipeline([\n",
    "        ('impute', num_impute),\n",
    "        ('encode', num_scale_encoder)\n",
    "    ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for feature transformation\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual feature-selection\n",
    "# We do a preselection of columns that we feel will become useful features after encoding\n",
    "if config.pre_feature_selection:\n",
    "    chosen_columns = ['prop_starrating', 'prop_review_score', 'prop_location_score1', 'prop_location_score2', \n",
    "                      'prop_log_historical_price', 'price_usd', 'srch_query_affinity_score',  'promotion_flag']\n",
    "    if config.feature_engineering:\n",
    "        chosen_columns = ['prop_starrating', 'prop_review_score', 'prop_mean_score', 'same_country_visitor_prop'\n",
    "                          'prop_log_historical_price', 'price_usd', 'srch_query_affinity_score',  'promotion_flag', \n",
    "                          'month', 'viable_comp', 'year']\n",
    "else:\n",
    "    chosen_columns = categorical_cols + numerical_cols\n",
    "\n",
    "for column in categorical_cols:\n",
    "    train_data[column]=train_data[column].astype('category')\n",
    "    \n",
    "    \n",
    "chosen_columns = list(set(chosen_columns) & set(train_data.columns))\n",
    "# Select the chosen columns, and \n",
    "# define the corresponding transformer's transformations to their columns\n",
    "chosen_oh_cols = list(set(chosen_columns) & set(categorical_cols))\n",
    "chosen_num_cols = list(set(chosen_columns) & set(numerical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_train_data = train_data[chosen_columns]\n",
    "\n",
    "df_transformer = ColumnTransformer([\n",
    "    ('oh', oh_pipeline, chosen_oh_cols),\n",
    "    ('num', num_pipeline, chosen_num_cols),\n",
    "], remainder='drop')\n",
    "\n",
    "# We fit this transformer on our training data, and transform/encode our training data\n",
    "encoded_X = df_transformer.fit_transform(chosen_train_data)\n",
    "\n",
    "# We also represent this same X using the original columns.\n",
    "new_oh_columns = df_transformer.named_transformers_.oh.named_steps.encode.get_feature_names(chosen_oh_cols)\n",
    "encoded_columns = [ *new_oh_columns, *chosen_num_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We extract the y-target in general\n",
    "X_only = train_data.copy()\n",
    "y = X_only.pop('booking_bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### We apply feature selection using the model from our config\n",
    "if config.dimensionality_reduc:\n",
    "    pca = TruncatedSVD(n_components=config.dimension_features)\n",
    "    feature_encoded_X = pca.fit_transform(encoded_X)\n",
    "else:\n",
    "    classifier = config.classifier(**config.classifier_dict)\n",
    "    feature_selector = config.feature_selection(classifier, **config.feature_selection_dict)\n",
    "    feature_encoded_X = feature_selector.fit_transform(encoded_X, y)\n",
    "    # TODO: support_ method might not work on every featureselector we choose, test this\n",
    "    bool_vec = feature_selector.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "srch_id                      False\n",
       "site_id                      False\n",
       "visitor_hist_starrating      False\n",
       "visitor_hist_adr_usd         False\n",
       "prop_id                      False\n",
       "prop_starrating              False\n",
       "prop_review_score            False\n",
       "prop_brand_bool              False\n",
       "prop_log_historical_price    False\n",
       "position                     False\n",
       "price_usd                    False\n",
       "promotion_flag               False\n",
       "srch_destination_id          False\n",
       "srch_length_of_stay          False\n",
       "srch_booking_window          False\n",
       "srch_adults_count            False\n",
       "srch_children_count          False\n",
       "srch_room_count              False\n",
       "srch_saturday_night_bool     False\n",
       "srch_query_affinity_score    False\n",
       "orig_destination_distance    False\n",
       "random_bool                  False\n",
       "click_bool                   False\n",
       "gross_bookings_usd           False\n",
       "booking_bool                 False\n",
       "month                        False\n",
       "year                         False\n",
       "same_country_visitor_prop    False\n",
       "viable_comp                  False\n",
       "prop_mean_score              False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isna().any()\n",
    "# Utility cell to investigate data elements\n",
    "# Data elements we have available\n",
    "# Encoded data:\n",
    "    # - df_encoded_X: Dataframe that contains the preprocessed features\n",
    "    # - encoded_X: numpy version of `encoded_df`\n",
    "# Original data:\n",
    "    # - train_data: training data, but cleaned up\n",
    "    # - X_only: `train_data` without `booking_bool`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try a various amount of models with parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "# Gets the sizes of same search-id chunks.\n",
    "get_user_groups_from_df = lambda df: df.groupby('srch_id').size().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign `srch_id`\n",
    "# feature_encoded_X['srch_id'] = train_data['srch_id'].astype(int)\n",
    "srch_id_col = train_data['srch_id'].astype(int)\n",
    "srch_id_col = np.array(srch_id_col)\n",
    "# srch_id_col = srch_id_col.reshape(-1,1)\n",
    "# print (srch_id_col.shape)\n",
    "# print(feature_encoded_X.shape)\n",
    "\n",
    "# feature_encoded_X_2 = np.vstack((feature_encoded_X, srch_id_col))\n",
    "# print (feature_encoded_X_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn-to-rank with LGBMRanker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we decide to split our data into train/val, we can do it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to rank!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# feature_encoded_X = encoded_X\n",
    "# Split data into 80% train and 20% validation, maintaining the groups however.\n",
    "train_inds, val_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 7).split(feature_encoded_X, groups=srch_id_col))\n",
    "\n",
    "# Split train / validation by their indices\n",
    "df_X_train = feature_encoded_X[train_inds]\n",
    "y_train = y[train_inds]\n",
    "df_X_val = feature_encoded_X[val_inds]\n",
    "y_val = y[val_inds]\n",
    "\n",
    "# Get the groups related to `srch_id`\n",
    "query_train = get_user_groups_from_df(train_data.iloc[train_inds])\n",
    "query_val = get_user_groups_from_df(train_data.iloc[val_inds])\n",
    "\n",
    "# Remove srch_id\n",
    "# df_X_train.pop('srch_id')\n",
    "# df_X_val.pop('srch_id')\n",
    "print(\"Ready to rank!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28     0\n",
       "29     0\n",
       "30     0\n",
       "31     0\n",
       "32     0\n",
       "      ..\n",
       "954    0\n",
       "955    0\n",
       "956    0\n",
       "957    0\n",
       "958    0\n",
       "Name: booking_bool, Length: 220, dtype: category\n",
       "Categories (2, int64): [0, 1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "for column in feature_encoded_X:\n",
    "    print (column.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#tidreassigning datatypes of x/y train/val (bool, np.array, to_list(), int) bugs gbm.fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Series.dtypes must be int, float or bool",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-a64037f8226f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m gbm.fit(df_X_train, y_train, group=query_train,\n\u001b[0;32m      5\u001b[0m         \u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_X_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_group\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mquery_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         eval_at=[5, 10, 20], early_stopping_rounds=50)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\thomas\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_group, eval_metric, eval_at, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    918\u001b[0m                                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m                                     \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m                                     callbacks=callbacks)\n\u001b[0m\u001b[0;32m    921\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thomas\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    598\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thomas\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_valid_set\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduced_valid_sets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_valid_sets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_valid_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reverse_update_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thomas\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36madd_valid\u001b[1;34m(self, data, name)\u001b[0m\n\u001b[0;32m   1893\u001b[0m         _safe_call(_LIB.LGBM_BoosterAddValidData(\n\u001b[0;32m   1894\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1895\u001b[1;33m             data.construct().handle))\n\u001b[0m\u001b[0;32m   1896\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1897\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_valid_sets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thomas\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1051\u001b[0m                                     \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m                                     \u001b[0minit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1053\u001b[1;33m                                     silent=self.silent, feature_name=self.feature_name, params=self.params)\n\u001b[0m\u001b[0;32m   1054\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m                     \u001b[1;31m# construct subset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thomas\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m    897\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot initialize Dataset from {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Label should not be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thomas\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mset_label\u001b[1;34m(self, label)\u001b[0m\n\u001b[0;32m   1384\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1386\u001b[1;33m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_to_1d_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_label_from_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1387\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_field\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1388\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_field\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# original values can be modified at cpp side\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thomas\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mlist_to_1d_numpy\u001b[1;34m(data, dtype, name)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_get_bad_pandas_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Series.dtypes must be int, float or bool'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# SparseArray should be supported as well\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Series.dtypes must be int, float or bool"
     ]
    }
   ],
   "source": [
    "# We define our ranker (default parameters)\n",
    "gbm = lgb.LGBMRanker()\n",
    "\n",
    "gbm.fit(df_X_train, y_train, group=query_train,\n",
    "        eval_set=[(df_X_val, y_val)], eval_group=[query_val],\n",
    "        eval_at=[5, 10, 20], early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "import os\n",
    "def ensure_path(path_to_file):\n",
    "    os.makedirs(os.path.dirname(path_to_file), exist_ok=True)\n",
    "\n",
    "ensure_path('storage/best_gbm.txt')\n",
    "gbm.booster_.save_model('storage/best_gbm.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data, encoded_X, feature_encoded_X, X_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from guppy import hpy\n",
    "gc.collect()\n",
    "h=hpy()\n",
    "h.heap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with LGBM-Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Read test data, and use the same columns as was used for training\n",
    "df_test_data = pd.read_csv('data/test_set_VU_DM.csv')\n",
    "chosen_test_data = df_test_data[chosen_columns]\n",
    "\n",
    "# Apply transformations (encoding + selection)\n",
    "encoded_test_data = df_transformer.transform(chosen_test_data)\n",
    "\n",
    "if config.PCA_use:\n",
    "    filtered_test_data = pca.transform(encoded_test_data)\n",
    "else:\n",
    "    filtered_test_data = encoded_test_data[:, bool_vec]\n",
    "                                    \n",
    "X_test = filtered_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting on a per-group basis: slow as hell (skip section for faster method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split test-data into groups based on the original data\n",
    "# groups = df_test_data.groupby('srch_id').indices\n",
    "# groups_by_idxs = list(groups.values())\n",
    "# print (groups_by_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "def predict_for_group(X_test, group_idxs, df_test_data):\n",
    "    # Use gbm to predict\n",
    "    X_test_group = X_test[group_idxs]\n",
    "    preds = gbm.predict(X_test_group)\n",
    "    preds = preds.argsort()[::-1] # Reverses\n",
    "    \n",
    "    # Get th\n",
    "    pred_idxs = group_idxs[preds]\n",
    "    pred_props = df_test_data.loc[pred_idxs, ['srch_id', 'prop_id']]\n",
    "    \n",
    "    return pred_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Doing it on a 'per-group basis'\n",
    "# Commented because it is slow.\n",
    "# # Perform the prediction (Can take a while, shitton of predictions)\n",
    "# result = []\n",
    "\n",
    "# for i, idx_group in enumerate(groups_by_idxs):\n",
    "#     preds = predict_for_group(X_test, idx_group, df_test_data)\n",
    "#     result.append(preds)\n",
    "    \n",
    "#     if i % 10000 == 0:\n",
    "#         print(f\"Doing group {i + 1} / {len(groups_by_idxs)} now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting all at once: More performant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all = gbm.predict(X_test)\n",
    "df_test_data['pred'] = pred_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort predictions based on srch_id and pred\n",
    "sorted_preds = df_test_data[['srch_id', 'prop_id', 'pred']].sort_values(by=['srch_id', 'pred'], ascending=[True, False]).reset_index()\n",
    "\n",
    "# Save\n",
    "sorted_preds[['srch_id', 'prop_id']].to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('results.csv', nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
