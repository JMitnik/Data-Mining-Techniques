{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Setting up\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import Lasso, RidgeClassifier, LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, RFE\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import LinearSVC, SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config parameters:\n",
    "\n",
    "nrows: default dataframelength: 4958347\n",
    "\n",
    "classifiers: (SVC, linearSVC, RidgeClassifier)\n",
    "    - SVC params: C (0-1), kernel (rbf, linear, poly), max_iter (-1, int), random_state (int)\n",
    "    - linearSVC params: C (0-1), penalty (l1, l2), max_iter (1000, int), random_state (int) \n",
    "    - RidgeClassifier params: max_iter (1000, int), random_state (int) \n",
    "    \n",
    "feature_selection: (SelectFromModel, SelectKBest, RFE)\n",
    "    - SelectFromModel params: threshold (0-int), max_features (0-int)\n",
    "    - SelectKBest params: threshold (0-int), k (0-int)\n",
    "    - RFE params: n_features_to_select (0-int), step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import config\n",
    "importlib.reload(config)\n",
    "from config import Config\n",
    "\n",
    "# Config Settings\n",
    "config = Config(\n",
    "    nrows=None,\n",
    "    pre_feature_selection=False, #todo Bug in prefeature selection = False\n",
    "    train_data_subset=0.8,\n",
    "    classifier=RidgeClassifier,\n",
    "    classifier_dict={'max_iter' : 1000, 'random_state' : 2},\n",
    "    feature_selection=RFE,\n",
    "    feature_selection_dict={'n_features_to_select' : 5, 'step' : 1} \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if config.nrows is not None:\n",
    "    train_data = pd.read_csv('data/training_set_VU_DM.csv', nrows=config.nrows)\n",
    "else:\n",
    "    train_data = pd.read_csv('data/training_set_VU_DM.csv')\n",
    "original_columns = train_data.columns\n",
    "train_data.head(5) # Show top 5\n",
    "train_data_nans = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #remove columns with over 50% nans\n",
    "# for column in train_data_nans.columns:\n",
    "#     if train_data_nans[column].isnull().sum()/len(train_data_nans) > 0.5:\n",
    "#         train_data_nans = train_data_nans.drop(columns=column, axis=1)\n",
    "        \n",
    "# train_data_nans.isnull().sum()/len(train_data_nans)\n",
    "# #remove data with > 0.50 nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in nans with mean values:\n",
    "# na_cols = train_data_nans.isna().any()\n",
    "# nan_cols = train_data_nans.columns[na_cols]\n",
    "# for column in nan_cols:\n",
    "#     print (column)\n",
    "#     if column in ['visitor_hist_starrating', 'visitor_hist_adr_usd',  \n",
    "#                      'srch_length_of_stay', 'srch_booking_window', \n",
    "#                      'srch_adults_count', 'srch_children_count',\n",
    "#                      'srch_room_count'                      \n",
    "#                     ]:\n",
    "#         train_data_nans[column] = train_data_nans.groupby('srch_id').transform(lambda x: x.fillna(x.mean()))\n",
    "#     elif column in ['prop_starrating', 'prop_review_score', \n",
    "#                        'prop_location_score1', 'prop_location_score2', \n",
    "#                        'prop_log_historical_price', 'price_usd',\n",
    "#                        'search_', 'orig_destination_distance',  \n",
    "#                        'srch_query_affinity_score'\n",
    "#                       ]:\n",
    "#         train_data_nans[column] = train_data_nans.groupby('prop_id').transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "    \n",
    "\n",
    "# train_data_nans.isnull().sum()/len(train_data_nans)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   srch_id            date_time  site_id  visitor_location_country_id  \\\n",
      "0        1  2013-04-04 08:32:15       12                          187   \n",
      "1        1  2013-04-04 08:32:15       12                          187   \n",
      "2        1  2013-04-04 08:32:15       12                          187   \n",
      "3        1  2013-04-04 08:32:15       12                          187   \n",
      "4        1  2013-04-04 08:32:15       12                          187   \n",
      "\n",
      "   visitor_hist_starrating  visitor_hist_adr_usd  prop_country_id  prop_id  \\\n",
      "0                      NaN                   NaN              219      893   \n",
      "1                      NaN                   NaN              219    10404   \n",
      "2                      NaN                   NaN              219    21315   \n",
      "3                      NaN                   NaN              219    27348   \n",
      "4                      NaN                   NaN              219    29604   \n",
      "\n",
      "   prop_starrating  prop_review_score  ...  comp6_rate_percent_diff  \\\n",
      "0                3                3.5  ...                      NaN   \n",
      "1                4                4.0  ...                      NaN   \n",
      "2                3                4.5  ...                      NaN   \n",
      "3                2                4.0  ...                      NaN   \n",
      "4                4                3.5  ...                      NaN   \n",
      "\n",
      "   comp7_rate  comp7_inv  comp7_rate_percent_diff  comp8_rate  comp8_inv  \\\n",
      "0         NaN        NaN                      NaN         0.0        0.0   \n",
      "1         NaN        NaN                      NaN         0.0        0.0   \n",
      "2         NaN        NaN                      NaN         0.0        0.0   \n",
      "3         NaN        NaN                      NaN        -1.0        0.0   \n",
      "4         NaN        NaN                      NaN         0.0        0.0   \n",
      "\n",
      "   comp8_rate_percent_diff  click_bool  gross_bookings_usd  booking_bool  \n",
      "0                      NaN           0                 NaN             0  \n",
      "1                      NaN           0                 NaN             0  \n",
      "2                      NaN           0                 NaN             0  \n",
      "3                      5.0           0                 NaN             0  \n",
      "4                      NaN           0                 NaN             0  \n",
      "\n",
      "[5 rows x 54 columns]\n"
     ]
    }
   ],
   "source": [
    "print (train_data_nans.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Column exploration\n",
    "---\n",
    "## Main columns\n",
    "- `search_id` seems to represent each individual 'user'.\n",
    "- `booking_bool` is essentially the answer.\n",
    "\n",
    "## Categorical features\n",
    "The following features are categorical (to be onehot-encoded):\n",
    "\n",
    "User-specific\n",
    "- `site_id`: category of website Expedia used\n",
    "- `visitor_location_country_id`: categories of which country user is from\n",
    "- `srch_destination_id`: where did the user search from\n",
    "- `srch_saturday_night_bool`: boolean if stay includes staturday\n",
    "\n",
    "Hotel-specific:\n",
    "- `prop_id`: categories of associated hotels\n",
    "- `prop_brand_bool`: boolean if hotel is part of chain or not\n",
    "- `promotion_flag`: displaying promotion or not\n",
    "\n",
    "Expedia-specific vs competitors 1_8:\n",
    "- `comp{i}_rate`: if expedia has a lower price, do +1, 0 if same, -1 price is higher, null if no competitive data\n",
    "- `comp{i}_inv`: if competitor has no availability, +1, 0 if both have availability, null if no competitive data\n",
    "\n",
    "## Numerical features\n",
    "\n",
    "User-specific\n",
    "- `visitor_hist_starrating`: average of previous stars of associated user\n",
    "- `visitor_hist_adr_usd`: average price per night of hotels of associated user\n",
    "- `srch_length_of_stay`: number of nights stays **searched** \n",
    "- `srch_booking_window`: number of days ahead the start of booking window **searched**\n",
    "- `srch_adults_count`: number of adults **searched**\n",
    "- `srch_children_count`: number of children **searched**\n",
    "- `srch_room_count`: number of rooms **searched**\n",
    "- `random_bool`: if sort was random at time of search\n",
    "- `gross_booking_usd`: ❗Training-only❗ payment includign taxes, etc for hotel\n",
    "\n",
    "Hotel-specific\n",
    "- `prop_starrating`: star rating of hotel (1-5)\n",
    "- `prop_review_score`: average review score of hotel (1-5)\n",
    "- `prop_location_score_1`: score1 of hotel's location desirability\n",
    "- `prop_location_score_2`: score2 of hotel's location desirability\n",
    "- `prop_log_historical_price`: logarithm of average price of hotel lately (0 == not sold)\n",
    "- `price_usd`: displayed price of hotel.\n",
    "    - ❗ Important: Different countries have different conventions.\n",
    "    - Value can change per night\n",
    "- `srch_query_affinity_score`: log probability a hotel is clicked in internet searches\n",
    "\n",
    "User-hotel coupled:\n",
    "- `orig_destination_distance`: distance between hotel and customer at search-time (null means no distance calculated)\n",
    "\n",
    "Expedia-specific vs competitors 1_8:\n",
    "- `comp{i}_rate_percent_diff`: absolute difference between expedia and competitor's price, with null being no competitive data\n",
    "\n",
    "\n",
    "## Unknown type\n",
    "- `date_time`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleanup: Imputing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['visitor_hist_starrating', 'visitor_hist_adr_usd', 'prop_review_score',\n",
       "       'prop_location_score2', 'srch_query_affinity_score',\n",
       "       'orig_destination_distance', 'comp1_rate', 'comp1_inv',\n",
       "       'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv',\n",
       "       'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv',\n",
       "       'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv',\n",
       "       'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv',\n",
       "       'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv',\n",
       "       'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv',\n",
       "       'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv',\n",
       "       'comp8_rate_percent_diff', 'gross_bookings_usd'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will have to cleanup our data next up. Let's first impute the missing columns. \n",
    "# To do this we search for the columns with nans\n",
    "na_cols = train_data.isna().any()\n",
    "nan_cols = train_data.columns[na_cols]\n",
    "nan_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from `comp{i}_rate` and `comp2_inv`, all of these columns are numerical features. We could, initially,\n",
    "simply replace all these values with -1 for the moment.\n",
    "\n",
    "❗ Important: Note, this is actually incorrect, but might work for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>date_time</th>\n",
       "      <th>site_id</th>\n",
       "      <th>visitor_location_country_id</th>\n",
       "      <th>visitor_hist_starrating</th>\n",
       "      <th>visitor_hist_adr_usd</th>\n",
       "      <th>prop_country_id</th>\n",
       "      <th>prop_id</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>...</th>\n",
       "      <th>comp6_rate_percent_diff</th>\n",
       "      <th>comp7_rate</th>\n",
       "      <th>comp7_inv</th>\n",
       "      <th>comp7_rate_percent_diff</th>\n",
       "      <th>comp8_rate</th>\n",
       "      <th>comp8_inv</th>\n",
       "      <th>comp8_rate_percent_diff</th>\n",
       "      <th>click_bool</th>\n",
       "      <th>gross_bookings_usd</th>\n",
       "      <th>booking_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>10404</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>21315</td>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>27348</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>29604</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   srch_id            date_time  site_id  visitor_location_country_id  \\\n",
       "0        1  2013-04-04 08:32:15       12                          187   \n",
       "1        1  2013-04-04 08:32:15       12                          187   \n",
       "2        1  2013-04-04 08:32:15       12                          187   \n",
       "3        1  2013-04-04 08:32:15       12                          187   \n",
       "4        1  2013-04-04 08:32:15       12                          187   \n",
       "\n",
       "   visitor_hist_starrating  visitor_hist_adr_usd  prop_country_id  prop_id  \\\n",
       "0                     -1.0                  -1.0              219      893   \n",
       "1                     -1.0                  -1.0              219    10404   \n",
       "2                     -1.0                  -1.0              219    21315   \n",
       "3                     -1.0                  -1.0              219    27348   \n",
       "4                     -1.0                  -1.0              219    29604   \n",
       "\n",
       "   prop_starrating  prop_review_score  ...  comp6_rate_percent_diff  \\\n",
       "0                3                3.5  ...                     -1.0   \n",
       "1                4                4.0  ...                     -1.0   \n",
       "2                3                4.5  ...                     -1.0   \n",
       "3                2                4.0  ...                     -1.0   \n",
       "4                4                3.5  ...                     -1.0   \n",
       "\n",
       "   comp7_rate  comp7_inv  comp7_rate_percent_diff  comp8_rate  comp8_inv  \\\n",
       "0         NaN        NaN                     -1.0         0.0        0.0   \n",
       "1         NaN        NaN                     -1.0         0.0        0.0   \n",
       "2         NaN        NaN                     -1.0         0.0        0.0   \n",
       "3         NaN        NaN                     -1.0        -1.0        0.0   \n",
       "4         NaN        NaN                     -1.0         0.0        0.0   \n",
       "\n",
       "   comp8_rate_percent_diff  click_bool  gross_bookings_usd  booking_bool  \n",
       "0                     -1.0           0                -1.0             0  \n",
       "1                     -1.0           0                -1.0             0  \n",
       "2                     -1.0           0                -1.0             0  \n",
       "3                      5.0           0                -1.0             0  \n",
       "4                     -1.0           0                -1.0             0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple numerical impute: select numerical data, fill it with -1\n",
    "imputed_numerical_data = train_data[nan_cols].filter(regex='[^comp\\d_(rate|inv)$]')\n",
    "imputed_numerical_data = imputed_numerical_data.fillna(-1)\n",
    "train_data.update(imputed_numerical_data)\n",
    "\n",
    "# Manual cleanup to ensure no problem with space\n",
    "del imputed_numerical_data\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>date_time</th>\n",
       "      <th>site_id</th>\n",
       "      <th>visitor_location_country_id</th>\n",
       "      <th>visitor_hist_starrating</th>\n",
       "      <th>visitor_hist_adr_usd</th>\n",
       "      <th>prop_country_id</th>\n",
       "      <th>prop_id</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>...</th>\n",
       "      <th>comp6_rate_percent_diff</th>\n",
       "      <th>comp7_rate</th>\n",
       "      <th>comp7_inv</th>\n",
       "      <th>comp7_rate_percent_diff</th>\n",
       "      <th>comp8_rate</th>\n",
       "      <th>comp8_inv</th>\n",
       "      <th>comp8_rate_percent_diff</th>\n",
       "      <th>click_bool</th>\n",
       "      <th>gross_bookings_usd</th>\n",
       "      <th>booking_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>10404</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>21315</td>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>27348</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>29604</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   srch_id            date_time  site_id  visitor_location_country_id  \\\n",
       "0        1  2013-04-04 08:32:15       12                          187   \n",
       "1        1  2013-04-04 08:32:15       12                          187   \n",
       "2        1  2013-04-04 08:32:15       12                          187   \n",
       "3        1  2013-04-04 08:32:15       12                          187   \n",
       "4        1  2013-04-04 08:32:15       12                          187   \n",
       "\n",
       "   visitor_hist_starrating  visitor_hist_adr_usd  prop_country_id  prop_id  \\\n",
       "0                     -1.0                  -1.0              219      893   \n",
       "1                     -1.0                  -1.0              219    10404   \n",
       "2                     -1.0                  -1.0              219    21315   \n",
       "3                     -1.0                  -1.0              219    27348   \n",
       "4                     -1.0                  -1.0              219    29604   \n",
       "\n",
       "   prop_starrating  prop_review_score  ...  comp6_rate_percent_diff  \\\n",
       "0                3                3.5  ...                     -1.0   \n",
       "1                4                4.0  ...                     -1.0   \n",
       "2                3                4.5  ...                     -1.0   \n",
       "3                2                4.0  ...                     -1.0   \n",
       "4                4                3.5  ...                     -1.0   \n",
       "\n",
       "   comp7_rate  comp7_inv  comp7_rate_percent_diff  comp8_rate  comp8_inv  \\\n",
       "0        -2.0       -2.0                     -1.0         0.0        0.0   \n",
       "1        -2.0       -2.0                     -1.0         0.0        0.0   \n",
       "2        -2.0       -2.0                     -1.0         0.0        0.0   \n",
       "3        -2.0       -2.0                     -1.0        -1.0        0.0   \n",
       "4        -2.0       -2.0                     -1.0         0.0        0.0   \n",
       "\n",
       "   comp8_rate_percent_diff  click_bool  gross_bookings_usd  booking_bool  \n",
       "0                     -1.0           0                -1.0             0  \n",
       "1                     -1.0           0                -1.0             0  \n",
       "2                     -1.0           0                -1.0             0  \n",
       "3                      5.0           0                -1.0             0  \n",
       "4                     -1.0           0                -1.0             0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple naive categorical impute\n",
    "na_cols = train_data.columns[train_data.isna().any()]\n",
    "imputed_categorical_data = train_data[na_cols].fillna(-2)\n",
    "train_data.update(imputed_categorical_data)\n",
    "\n",
    "# Cleanup\n",
    "del imputed_categorical_data\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for feature transformation\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Here we definehow we would like to encode\n",
    "\n",
    "# For One-Hot Encoding\n",
    "# Onehot encode the categorical variables\n",
    "oh_columns = ['site_id', 'visitor_location_country_id', 'prop_country_id', \n",
    "              'prop_id', 'prop_brand_bool', 'promotion_flag', \n",
    "              'srch_destination_id', 'srch_saturday_night_bool', 'random_bool', 'click_bool'\n",
    "             ]\n",
    "oh_impute = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=-2)\n",
    "oh_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "oh_pipeline = Pipeline([\n",
    "    ('impute', oh_impute),\n",
    "    ('encode', oh_encoder)\n",
    "])\n",
    "# TODO: competitor columns\n",
    "for column in oh_columns:\n",
    "    train_data[column]=train_data[column].astype('category')\n",
    "\n",
    "\n",
    "# Encode the numerical values\n",
    "num_scale_columns = ['visitor_hist_starrating', 'visitor_hist_adr_usd', \n",
    "                     'prop_starrating', 'prop_review_score', \n",
    "                     'prop_location_score1', 'prop_location_score2', \n",
    "                     'prop_log_historical_price', 'price_usd', \n",
    "                     'srch_length_of_stay', 'srch_booking_window', \n",
    "                     'srch_adults_count', 'srch_children_count',\n",
    "                     'srch_room_count', 'srch_query_affinity_score', \n",
    "                     'orig_destination_distance' \n",
    "                    ]\n",
    "num_impute = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=-1)\n",
    "num_scale_encoder = StandardScaler()\n",
    "num_pipeline = Pipeline([\n",
    "    ('impute', num_impute),\n",
    "    ('encode', num_scale_encoder)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual feature-selection\n",
    "# We do a preselection of columns that we feel will become useful features after encoding\n",
    "if config.pre_feature_selection == True:\n",
    "    chosen_columns = ['prop_starrating', 'prop_review_score', 'prop_location_score1', 'prop_location_score2', \n",
    "                      'prop_log_historical_price', 'price_usd', 'srch_query_affinity_score',  'promotion_flag']\n",
    "else:\n",
    "    chosen_columns = oh_columns + num_scale_columns\n",
    "\n",
    "# Select the chosen columns, and \n",
    "# define the corresponding transformer's transformations to their columns\n",
    "chosen_oh_cols = list(set(chosen_columns) & set(oh_columns))\n",
    "chosen_num_cols = list(set(chosen_columns) & set(num_scale_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (4958347, 1), indices imply (4958347, 147681)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\thomas\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[1;34m(blocks, axes)\u001b[0m\n\u001b[0;32m   1653\u001b[0m                 blocks = [\n\u001b[1;32m-> 1654\u001b[1;33m                     \u001b[0mmake_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1655\u001b[0m                 ]\n",
      "\u001b[1;32mc:\\users\\thomas\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[1;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[0;32m   3046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3047\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thomas\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, values, placement, ndim)\u001b[0m\n\u001b[0;32m   2594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2595\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thomas\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, values, placement, ndim)\u001b[0m\n\u001b[0;32m    124\u001b[0m             raise ValueError(\n\u001b[1;32m--> 125\u001b[1;33m                 \u001b[1;34mf\"Wrong number of items passed {len(self.values)}, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m                 \u001b[1;34mf\"placement implies {len(self.mgr_locs)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Wrong number of items passed 1, placement implies 147681",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-5cd75acfbe72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mnew_oh_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_transformers_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchosen_oh_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mencoded_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mnew_oh_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mchosen_num_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mdf_encoded_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoded_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\thomas\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    486\u001b[0m                     \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m                     \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thomas\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[1;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0mblock_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thomas\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[1;34m(blocks, axes)\u001b[0m\n\u001b[0;32m   1662\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"values\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1663\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1664\u001b[1;33m         \u001b[0mconstruction_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thomas\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[1;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[0;32m   1692\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mblock_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1694\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (4958347, 1), indices imply (4958347, 147681)"
     ]
    }
   ],
   "source": [
    "chosen_train_data = train_data[chosen_columns]\n",
    "\n",
    "df_transformer = ColumnTransformer([\n",
    "    ('oh', oh_pipeline, chosen_oh_cols),\n",
    "    ('num', num_pipeline, chosen_num_cols),\n",
    "], remainder='drop')\n",
    "\n",
    "# We fit this transformer on our training data, and transform/encode our training data\n",
    "encoded_X = df_transformer.fit_transform(chosen_train_data)\n",
    "\n",
    "# We also represent this same X using the original columns.\n",
    "new_oh_columns = df_transformer.named_transformers_.oh.named_steps.encode.get_feature_names(chosen_oh_cols)\n",
    "encoded_columns = [ *new_oh_columns, *chosen_num_cols]\n",
    "df_encoded_X = pd.DataFrame(encoded_X, columns=encoded_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We extract the y-target in general\n",
    "X_only = train_data.copy()\n",
    "y = X_only.pop('booking_bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### We apply feature selection using the model from our config\n",
    "classifier = config.classifier(**config.classifier_dict)\n",
    "feature_selector = config.feature_selection(classifier, **config.feature_selection_dict)\n",
    "feature_encoded_X = feature_selector.fit_transform(df_encoded_X, y)\n",
    "# TODO: support_ method might not work on every featureselector we choose, test this\n",
    "bool_vec = feature_selector.support_\n",
    "feature_cols = np.array(encoded_columns)[bool_vec]\n",
    "df_feature_encoded = pd.DataFrame(feature_encoded_X, columns=feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility cell to investigate data elements\n",
    "# Data elements we have available\n",
    "# Encoded data:\n",
    "    # - df_encoded_X: Dataframe that contains the preprocessed features\n",
    "    # - encoded_X: numpy version of `encoded_df`\n",
    "# Original data:\n",
    "    # - train_data: training data, but cleaned up\n",
    "    # - X_only: `train_data` without `booking_bool`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try a various amount of models with parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "# Gets the sizes of same search-id chunks.\n",
    "get_user_groups_from_df = lambda df: df.groupby('srch_id').size().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign `srch_id`\n",
    "df_feature_encoded['srch_id'] = train_data['srch_id'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn-to-rank with LGBMRanker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we decide to split our data into train/val, we can do it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Split data into 80% train and 20% validation, maintaining the groups however.\n",
    "train_inds, val_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 7).split(df_feature_encoded, groups=df_feature_encoded['srch_id']))\n",
    "\n",
    "# Split train / validation by their indices\n",
    "df_X_train = df_feature_encoded.iloc[train_inds]\n",
    "y_train = y[train_inds]\n",
    "df_X_val = df_feature_encoded.iloc[val_inds]\n",
    "y_val = y[val_inds]\n",
    "\n",
    "# Get the groups related to `srch_id`\n",
    "query_train = get_user_groups_from_df(df_X_train)\n",
    "query_val = get_user_groups_from_df(df_X_val)\n",
    "\n",
    "# Remove srch_id\n",
    "df_X_train.pop('srch_id')\n",
    "df_X_val.pop('srch_id')\n",
    "print(\"Ready to rank!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our ranker (default parameters)\n",
    "gbm = lgb.LGBMRanker()\n",
    "\n",
    "gbm.fit(df_X_train, y_train, group=query_train,\n",
    "        eval_set=[(df_X_val, y_val)], eval_group=[query_val],\n",
    "        eval_at=[5, 10, 20], early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "import os\n",
    "def ensure_path(path_to_file):\n",
    "    os.makedirs(os.path.dirname(path_to_file), exist_ok=True)\n",
    "\n",
    "ensure_path('storage/best_gbm.txt')\n",
    "gbm.booster_.save_model('storage/best_gbm.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with LGBM-Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data, and use the same columns as was used for training\n",
    "df_test_data = pd.read_csv('data/test_set_VU_DM.csv')\n",
    "chosen_test_data = df_test_data[chosen_columns]\n",
    "\n",
    "# Apply transformations (encoding + selection)\n",
    "encoded_test_data = df_transformer.transform(chosen_test_data)\n",
    "df_encoded_test_data = pd.DataFrame(encoded_test_data, columns=encoded_columns)\n",
    "\n",
    "feature_cols = (list(feature_cols))\n",
    "filtered_test_data = df_encoded_test_data[feature_cols]\n",
    "                                    \n",
    "X_test = filtered_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting on a per-group basis: slow as hell (skip section for faster method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test-data into groups based on the original data\n",
    "groups = df_test_data.groupby('srch_id').indices\n",
    "groups_by_idxs = list(groups.values())\n",
    "print (groups_by_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "def predict_for_group(X_test, group_idxs, df_test_data):\n",
    "    # Use gbm to predict\n",
    "    X_test_group = X_test[group_idxs]\n",
    "    preds = gbm.predict(X_test_group)\n",
    "    preds = preds.argsort()[::-1] # Reverses\n",
    "    \n",
    "    # Get th\n",
    "    pred_idxs = group_idxs[preds]\n",
    "    pred_props = df_test_data.loc[pred_idxs, ['srch_id', 'prop_id']]\n",
    "    \n",
    "    return pred_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Doing it on a 'per-group basis'\n",
    "# Commented because it is slow.\n",
    "# # Perform the prediction (Can take a while, shitton of predictions)\n",
    "# result = []\n",
    "\n",
    "# for i, idx_group in enumerate(groups_by_idxs):\n",
    "#     preds = predict_for_group(X_test, idx_group, df_test_data)\n",
    "#     result.append(preds)\n",
    "    \n",
    "#     if i % 10000 == 0:\n",
    "#         print(f\"Doing group {i + 1} / {len(groups_by_idxs)} now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting all at once: More performant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all = gbm.predict(X_test)\n",
    "df_test_data['pred'] = pred_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort predictions based on srch_id and pred\n",
    "sorted_preds = df_test_data[['srch_id', 'prop_id', 'pred']].sort_values(by=['srch_id', 'pred'], ascending=[True, False]).reset_index()\n",
    "\n",
    "# Save\n",
    "sorted_preds[['srch_id', 'prop_id']].to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('results.csv', nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
